FROM nvidia/cuda:13.0.1-cudnn-devel-ubuntu24.04 AS build

ARG VERSION=stream-fix
ARG MAX_JOBS=64

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    git \
    python3 \
    python3-dev \
    python3-venv \
    python3-pip && \
    rm -rf /var/lib/apt/lists/*

ENV MAX_JOBS=${MAX_JOBS}

RUN python3 -m venv /opt/venv && \
    . /opt/venv/bin/activate && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir ninja packaging numpy && \
    pip install torch==2.9.0 --index-url https://download.pytorch.org/whl/cu130 && \
    git clone https://github.com/kevmo314/flash-attention && \
    cd flash-attention/hopper && \
    git checkout ${VERSION} && \
    python3 setup.py bdist_wheel

FROM scratch

COPY --from=build /flash-attention/hopper/dist/* .
