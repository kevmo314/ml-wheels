FROM nvidia/cuda:13.0.0-cudnn-devel-ubuntu24.04 AS build

ARG VERSION=91f14ca07b792645b72efbb05b233907a831c898
ARG MAX_JOBS=16

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    git \
    python3 \
    python3-dev \
    python3-venv \
    python3-pip && \
    rm -rf /var/lib/apt/lists/*

ENV MAX_JOBS=${MAX_JOBS}

RUN python3 -m venv /opt/venv && \
    . /opt/venv/bin/activate && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir ninja packaging numpy && \
    pip install torch==2.9.0 --index-url https://download.pytorch.org/whl/cu130 && \
    git clone https://github.com/Dao-AILab/flash-attention && \
    cd flash-attention/hopper && \
    git checkout ${VERSION} && \
    sed -i '/if bare_metal_version < Version("12.3"):/,/raise RuntimeError/c\    if bare_metal_version < Version("12.3"):\n        raise RuntimeError("FlashAttention-3 is only supported on CUDA 12.3 and above")\n    elif bare_metal_version >= Version("13.0"):\n        pass  # CUDA 13.0+ uses system nvcc' setup.py && \
    sed -i 's/if bare_metal_version != Version("12.8"):/if bare_metal_version >= Version("12.3") and bare_metal_version < Version("13.0") and bare_metal_version != Version("12.8"):/' setup.py && \
    CPLUS_INCLUDE_PATH=/usr/local/cuda/include/cccl C_INCLUDE_PATH=/usr/local/cuda/include/cccl python3 setup.py bdist_wheel

FROM scratch

COPY --from=build /flash-attention/hopper/dist/* .
