FROM nvidia/cuda:12.8.1-cudnn-devel-ubuntu24.04 AS build

ARG VERSION=98edb0d29bb1db336fef845fb5fd49bc98b04b96

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    git \
    python3 \
    python3-dev \
    python3-venv \
    python3-pip && \
    rm -rf /var/lib/apt/lists/*

RUN python3 -m venv /opt/venv && \
    . /opt/venv/bin/activate && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir ninja packaging numpy && \
    pip install torch --index-url https://download.pytorch.org/whl/cu128 && \
    git clone https://github.com/Dao-AILab/flash-attention && \
    cd flash-attention/hopper && \
    git checkout ${VERSION} && \
    MAX_JOBS=16 python3 setup.py bdist_wheel

FROM scratch

COPY --from=build /flash-attention/hopper/dist/* .