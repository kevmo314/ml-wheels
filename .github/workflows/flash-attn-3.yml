name: github.com/Dao-AILab/flash-attention
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    strategy:
      matrix:
        version: [main]
        os: [ubuntu-24.04, ubuntu-24.04-arm]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/setup-python@v5
        with:
          python-version: '3.13'
      - name: Install CUDA Toolkit (Ubuntu x86-64)
        if: matrix.os == 'ubuntu-24.04'
        run: |
          wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
          sudo dpkg -i cuda-keyring_1.1-1_all.deb
          sudo apt-get update
          sudo apt-get -y install cuda-toolkit-12-8
      - name: Install CUDA Toolkit (Ubuntu arm64)
        if: matrix.os == 'ubuntu-24.04-arm'
        run: |
          wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/sbsa/cuda-keyring_1.1-1_all.deb
          sudo dpkg -i cuda-keyring_1.1-1_all.deb
          sudo apt-get update
          sudo apt-get -y install cuda-toolkit-12-8
      - run: |
          git clone https://github.com/Dao-AILab/flash-attention
          cd flash-attention/hopper
          pip install ninja packaging setuptools
          pip install torch --index-url https://download.pytorch.org/whl/cu128
          MAX_JOBS=2 python setup.py bdist_wheel
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: flash-attention-${{ matrix.os }}-${{ matrix.version }}
          path: |
            flash-attention/hopper/dist/*.whl
