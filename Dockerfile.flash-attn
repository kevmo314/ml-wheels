FROM nvidia/cuda:13.0.0-cudnn-devel-ubuntu24.04 AS build

ARG VERSION=v2.8.3
ARG MAX_JOBS=16

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    git \
    python3 \
    python3-dev \
    python3-venv \
    python3-pip && \
    rm -rf /var/lib/apt/lists/*

ENV MAX_JOBS=${MAX_JOBS}

RUN python3 -m venv /opt/venv && \
    . /opt/venv/bin/activate && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir ninja packaging numpy && \
    pip install torch --index-url https://download.pytorch.org/whl/cu130 && \
    git clone https://github.com/Dao-AILab/flash-attention && \
    cd flash-attention && \
    git checkout ${VERSION} && \
    python3 setup.py bdist_wheel

FROM scratch

COPY --from=build /flash-attention/dist/* .
